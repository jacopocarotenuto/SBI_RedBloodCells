{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMULATOR AND SUMMARY STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the minimum required library to run the functions\n",
    "from Utils_functions import *\n",
    "from numpy.random import uniform\n",
    "from numpy import array, int64, where, concatenate, linspace, median, sum, log, zeros, mean, var, min, max, einsum\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "from scipy.signal import welch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 1e-6# The time step size for the simulation. This is the amount of time that passes in the model for each step of the simulation\n",
    "Sample_frequency = 25_000 # The sampling frequency. This is the number of samples that are recorded per unit time\n",
    "DeltaT = 1/Sample_frequency # The sampling period. This is the amount of time that passes in the model between each sample that is recorded\n",
    "TotalT = 1 # The total time for the simulation. This is the total amount of time that the simulation is intended to represent\n",
    "n_sim = int(2e3)\n",
    "t_corr = TotalT/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps_amount = int64(TotalT/dt) # Number of steps\n",
    "sampled_point_amount = int64(TotalT/DeltaT) # Number of sampled points\n",
    "sampling_delta_time_steps = int64(DeltaT/dt) # Number of steps between samples\n",
    "t = linspace(0., TotalT,sampled_point_amount) # Time array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chose the time characteristics of the input signal (total time, sample rate, numbero of simulations) and transform them into points info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limits for the parameters\n",
    "prior_limits = [[1.5e4, 4e4], \n",
    "                [1e4, 140e4], \n",
    "                [3e-3, 16e-3], \n",
    "                [1.5e-2, 30e-2], \n",
    "                [1e-3, 6e-3], \n",
    "                [2e-2, 20e-2], \n",
    "                [0.5, 6], \n",
    "                [5.5, 15.5], \n",
    "                [1, 530]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the theta values from the prior in the different shapes\n",
    "theta, theta_torch, prior_box = get_theta_from_prior(prior_limits, n_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy call to force compilation\n",
    "theta, theta_torch, prior_box = get_theta_from_prior(prior_limits, 1)\n",
    "Simulator_noGPU(0.1,0.1,1,1,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "theta, theta_torch, prior_box = get_theta_from_prior(prior_limits, n_sim)\n",
    "x_trace, f_trace, y_trace = Simulator_noGPU(dt, DeltaT, TotalT, n_sim, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import _pickle as pickle\n",
    "# with open('data/x_trace.pkl', 'wb') as f:\n",
    "#     pickle.dump(x_trace, f)\n",
    "    \n",
    "# with open('data/f_trace.pkl', 'wb') as f:\n",
    "#     pickle.dump(f_trace, f)\n",
    "\n",
    "# with open('data/y_trace.pkl', 'wb') as f:\n",
    "#     pickle.dump(y_trace, f)\n",
    "    \n",
    "# with open(\"data/theta.pkl\", 'wb') as f:\n",
    "#     pickle.dump(theta, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_mean, Fx, Fy, S_tot = compute_entropy_production(x_trace, y_trace, f_trace, theta, n_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plt.plot(x_trace[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulating the x trajectory and the f trajectory to then obtain the summary statistics, in this case the autocorrellation and the cross correllation in combination with the reduced energy production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know which formula we can pick to compute the reduce energy production, from autocorrellation, from cross correllation, all together?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD: Write a function GET_SUMMARY_STATISTICS() which is a list of functions, each function for a summoner statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "list_stat = [\"Cxx\", \"s_redx\", \"s_redf\", \"psdx\"]\n",
    "s = get_summary_statistics(list_stat, x_trace, f_trace, theta, DeltaT, 3, t, t_corr)\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we can select different summary statistics and usa some feature selection technique. If we use a lot of summary statistics maybe dimentionality reductions? PCA?\n",
    "The idea is to use different summary statistics but to then to select the most important features of them deleting the correllations between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example could be the Power spectral density for both x and f.\n",
    "It can identify if there are any dominant frequencies in your data, or if the power is distributed across a range of frequencies, usefull for oscillatory behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use directly the psds as input of the neural network or we can take some statistics about each one, like mean, median, max, min,variance, entropy, frequency of top power. We can think of more statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, Size\n",
    "from sbi import analysis as analysis\n",
    "from sbi import utils as utils\n",
    "from sbi.inference import SNPE, simulate_for_sbi, infer\n",
    "from sbi.utils.user_input_checks import (\n",
    "    check_sbi_inputs,\n",
    "    process_prior,\n",
    "    process_simulator,\n",
    ")\n",
    "from numpy import squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior distribution for sbi\n",
    "prior, num_parameters, prior_returns_numpy = process_prior(prior_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call infer form sbi\n",
    "infer = SNPE(prior=prior)\n",
    "inferece = infer.append_simulations(theta_torch, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "density_estimator = infer.train()\n",
    "posterior = infer.build_posterior(density_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the observation(s)\n",
    "theta_true, theta_torch_true, prior_box_true = get_theta_from_prior(prior_limits, n_sim=1)\n",
    "x_trace_true, f_trace_true, y_trace_true = Simulator_noGPU(dt, DeltaT, TotalT, 1, theta_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the posterior\n",
    "s = get_summary_statistics(list_stat, x_trace_true, f_trace_true, theta, DeltaT, 3, t, t_corr)\n",
    "samples = posterior.sample((int(1e5),), x=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the some statistics from each marginal distribution\n",
    "mean_params = np.array([])\n",
    "mode_params = np.array([])\n",
    "\n",
    "for i in range(9):\n",
    "    # Retrive the samples for the parameter i\n",
    "    params = samples[:,i].numpy()\n",
    "    # Compute the mean\n",
    "    mean_params = np.concatenate((mean_params, [np.mean(params)]))\n",
    "    # Compute the mode\n",
    "    hist, bin_edges = np.histogram(params, bins=int(np.sqrt(params.shape[0])))\n",
    "    max_index = np.argmax(hist)\n",
    "    mode = (bin_edges[max_index] + bin_edges[max_index+1])/2\n",
    "    mode_params = np.concatenate((mode_params, [mode]))\n",
    "\n",
    "mean_params = mean_params.reshape(9, 1, 1)\n",
    "mode_params = mode_params.reshape(9, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "label_theta = [r\"$\\mu_x$\", r\"$\\mu_y$\", r\"$k_x$\", r\"$k_y$\", r\"$k_{int}$\", r\"$\\tau$\", r\"$\\varepsilon$\", r\"$D_x$\", r\"$D_y$\"]\n",
    "import matplotlib as mpl\n",
    "_ = analysis.pairplot(samples, points=[theta_torch_true], \n",
    "                      limits=prior_limits, figsize=(10, 10), \n",
    "                      title = \"Pairplot for the marginal posterior\",\n",
    "                      labels=label_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a new trace using the mode parameters\n",
    "x_trace_est, f_trace_est, y_trace_est = Simulator_noGPU(dt, DeltaT, TotalT, 1, mode_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTROPY PRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the formulas used to compute the entropy production\n",
    "$$\n",
    "S = \\sum_{t=0}^{N_{\\text{timesteps}}-2} \\left( \\text{data}_{t+1} - \\text{data}_{t} \\right) \\cdot F\\left( \\frac{\\text{data}_{t+1} + \\text{data}_{t}}{2} \\right) \\cdot D^{-1}\\left( \\frac{\\text{data}_{t+1} + \\text{data}_{t}}{2} \\right)\n",
    "$$\n",
    "$$\n",
    "\\sigma = \\frac{{\\mu_y \\cdot \\varepsilon^2}}{{1 + k_y \\cdot \\mu_y \\cdot \\tau - \\frac{{k_{int}^2 \\cdot \\mu_x \\cdot \\mu_y \\cdot \\tau^2}}{{1 + k_x \\cdot \\mu_x \\cdot \\tau}}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_true, _, _, _ = compute_entropy_production(x_trace_true, y_trace_true, f_trace_true, theta_true, 1)\n",
    "sigma_est, _, _, _ = compute_entropy_production(x_trace_est, y_trace_est, f_trace_est, mean_params, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sigma_true, sigma_est)\n",
    "sigma_true/sigma_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_true_2, _, = compute_entropy_2(theta_true, 1)\n",
    "sigma_est_2_mean, _, = compute_entropy_2(mean_params, 1)\n",
    "sigma_est_2_mode, _, = compute_entropy_2(mode_params, 1)\n",
    "print(\"Estimates for the analytical entropy production:\")\n",
    "print(f\"True: %.2f, Mean: %.2f, Mode: %.2f\" % (sigma_true_2[0][0], sigma_est_2_mean[0][0], sigma_est_2_mode[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_true_2, sigma_est_2_mode, sigma_est_2_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the statistics of sigma directly out of the samples\n",
    "sigma_samples = array([])\n",
    "#sanity_check = array([])\n",
    "\n",
    "for i in range(samples.shape[0]):\n",
    "    sample = samples[i,:].numpy().reshape(9,1,1)\n",
    "    #sanity_check = concatenate((sanity_check, [sample[0][0][0]]))\n",
    "    sigma = compute_entropy_2(list(sample), 1)[0][0][0]\n",
    "    sigma_samples = concatenate((sigma_samples, [sigma]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlim(0, 1e4)\n",
    "plt.vlines(sigma_true_2, 0, 1e3, color='r')\n",
    "plt.xlabel(r\"$\\sigma$\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Sigma from sampling\")\n",
    "plt.hist(sigma_samples, bins=int(np.sqrt(len(sigma_samples))));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTIONS\n",
    "- per calcolare S_red come summary statistics (come fatto da Dresda nel file) usiamo i parametri, non è un bias poi usarla come summary statistics nel neural network durante la SBI?\n",
    "- qual è la formula corretta per l'entropia? Cosa abbiamo usato noi?\n",
    "- limiti per la prior distribution sui parametri"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
